import streamlit as st
import pdfplumber
import pandas as pd
import re
import threading
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM,
    TextIteratorStreamer,
)
from fpdf import FPDF

# ------------------------------
# CONFIG
# ------------------------------
HF_TOKEN = st.secrets["huggingface"]["token"]

st.set_page_config(page_title="SpendSense: AIâ€‘Powered Financial Coach", page_icon="ðŸ’°", layout="wide")


# ------------------------------
# THEME & BACKGROUND
# ------------------------------
page_bg_css = """
<style>
/* Import a modern Google Font */
@import url('https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap');

/* Full-app background with dark overlay */
[data-testid="stAppViewContainer"] {
  background-image:
    linear-gradient(rgba(0,0,0,0.35), rgba(0,0,0,0.35)),
    url("bg.png");
  background-size: cover;
  background-position: center;
  background-repeat: no-repeat;
  font-family: 'Poppins', sans-serif;
  animation: fadeIn 0.8s ease-in-out;
}

/* Transparent top bar */
[data-testid="stHeader"] { background: rgba(0,0,0,0); }

/* Main content container with glass effect */
.block-container {
  background: rgba(255, 255, 255, 0.85);
  backdrop-filter: blur(6px);
  border-radius: 14px;
  padding: 1.5rem;
  box-shadow: 0 4px 20px rgba(0,0,0,0.15);
  animation: slideUp 0.6s ease-out;
}

/* Headings */
.block-container h1 {
  color: #111;
  letter-spacing: 0.5px;
  margin-bottom: 0.5rem;
  font-weight: 600;
}
.block-container h2, .block-container h3 {
  color: #222;
  font-weight: 600;
}

/* Buttons */
div.stButton > button {
  background-color: #ff9800;
  color: white;
  border-radius: 8px;
  font-weight: bold;
  padding: 0.5rem 1rem;
  border: none;
  transition: background-color 0.3s ease;
}
div.stButton > button:hover {
  background-color: #e68900;
}

/* File uploader styling */
[data-testid="stFileUploader"] {
  background: rgba(255,255,255,0.9);
  border-radius: 10px;
  padding: 1rem;
  border: 2px dashed #ff9800;
}

/* Animations */
@keyframes fadeIn {
  from { opacity: 0; }
  to { opacity: 1; }
}
@keyframes slideUp {
  from { transform: translateY(10px); opacity: 0; }
  to { transform: translateY(0); opacity: 1; }
}
</style>
"""

st.markdown(page_bg_css, unsafe_allow_html=True)

# ------------------------------
# SESSION STATE
# ------------------------------
if "tokenizer" not in st.session_state:
    st.session_state.tokenizer = None
if "model" not in st.session_state:
    st.session_state.model = None
if "model_choice" not in st.session_state:
    st.session_state.model_choice = None
if "df" not in st.session_state:
    st.session_state.df = None

# ------------------------------
# MODEL LOADING HELPERS (GPU if available)
# ------------------------------
@st.cache_resource
def load_causal_model(model_id: str):
    device = 0 if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        token=HF_TOKEN,
        trust_remote_code=True,
        device_map="auto" if torch.cuda.is_available() else None,
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else None
    ).to(device)
    return tokenizer, model

@st.cache_resource
def load_seq2seq_model(model_id: str):
    device = 0 if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_id,
        token=HF_TOKEN,
        device_map="auto" if torch.cuda.is_available() else None,
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else None
    ).to(device)
    return tokenizer, model

# ------------------------------
# PDF GENERATOR
# ------------------------------
def generate_advice_pdf(text: str) -> bytes:
    pdf = FPDF(orientation="P", unit="mm", format="A4")
    pdf.add_page()
    pdf.set_auto_page_break(auto=True, margin=15)

    # Title
    pdf.set_font("Arial", 'B', 16)
    pdf.cell(0, 10, "SpendSense: AIâ€‘Powered Financial Coach", ln=True, align="C")
    pdf.ln(5)

    # Body
    pdf.set_font("Arial", size=12)
    for line in text.split("\n"):
        for chunk in [line[i:i+90] for i in range(0, len(line), 90)]:
            pdf.cell(0, 6, chunk, ln=True)

    return pdf.output(dest="S").encode("latin-1")
# ------------------------------
# STREAMING GENERATION
# ------------------------------
def stream_generate(model, tokenizer, prompt, max_new_tokens=256):
    device = 0 if torch.cuda.is_available() else "cpu"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
    generation_kwargs = dict(**inputs, max_new_tokens=max_new_tokens, streamer=streamer)
    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    for new_text in streamer:
        yield new_text

# ------------------------------
# PDF UPLOAD & PARSING
# ------------------------------
st.header("ðŸ“‚ Upload Your Transaction PDF")
uploaded_pdf = st.file_uploader("Upload your bank/payment statement (PDF)", type="pdf")

if uploaded_pdf is not None:
    with pdfplumber.open(uploaded_pdf) as pdf:
        full_text = "\n".join(page.extract_text() or "" for page in pdf.pages)

    pattern = re.compile(
        r"(Received|Paid) â‚¹([\d,]+\.\d{2})"
        r"(?: to (.*?))?\s*\n"
        r"([A-Za-z]{3} \d{1,2}, \d{4}, \d{1,2}:\d{2}:\d{2} [APM]+) GMT"
        r".*?Details:\s*([\w/]+)\s*\n"
        r"(Completed|Failed)",
        re.DOTALL
    )

    matches = pattern.findall(full_text)
    records = []
    for ttype, amt_str, recv, ts_str, txid, status in matches:
        dt = pd.to_datetime(ts_str, format="%b %d, %Y, %I:%M:%S %p", errors="coerce")
        amt = float(amt_str.replace(",", ""))
        recv = recv or "Self (Received)"
        cat = "Income" if ttype == "Received" else "Expense"
        records.append({
            "datetime": dt,
            "transaction_type": ttype,
            "is_income": ttype == "Received",
            "is_expense": ttype == "Paid",
            "amount": amt,
            "merchant": recv,
            "transaction_id": txid,
            "status": status,
            "spend_category": "Other"
        })

    df = pd.DataFrame(records).dropna(subset=["datetime"])
    st.session_state.df = df
    st.success(f"âœ… Parsed {len(df)} transactions from your PDF")
    st.dataframe(df.head())

# ------------------------------
# MODEL UI
# ------------------------------
st.title("ðŸ’° SpendSense: AIâ€‘Powered Financial Coach")

# Auto-select default model based on GPU availability
if torch.cuda.is_available():
    default_model = "mistralai/Mistral-7B-Instruct-v0.1"
else:
    default_model = "tiiuae/falcon-7b-instruct"

model_choice = st.selectbox(
    "Select model",
    [
        "tiiuae/falcon-7b-instruct",   # Best quality on CPU
        "microsoft/phi-2",             # Fastest
        "google/gemma-2b-it",          # Balanced
        "google/flan-t5-large",        # Fallback
        "mistralai/Mistral-7B-Instruct-v0.1"  # Only practical on GPU
    ],
    index=[ 
        0 if not torch.cuda.is_available() else 4  # Falcon on CPU, Mistral on GPU
    ][0]
)
st.session_state.model_choice = model_choice

load_clicked = st.button("Load model")
if load_clicked:
    with st.spinner(f"Loading '{model_choice}'..."):
        if "mistral" in model_choice.lower() or "falcon" in model_choice.lower():
            tok, mdl = load_causal_model(model_choice)
        else:
            tok, mdl = load_seq2seq_model(model_choice)
        st.session_state.tokenizer = tok
        st.session_state.model = mdl
    st.success(f"Model '{model_choice}' loaded âœ…")
    
user_input = st.text_area("Enter your question or request for advice:")

# ------------------------------
# GENERATE ADVICE
# ------------------------------
generate_disabled = st.session_state.model is None
gen_clicked = st.button("Generate advice", disabled=generate_disabled)

if gen_clicked:
    if st.session_state.df is None:
        st.warning("Please upload a PDF first.")
    elif not user_input.strip():
        st.warning("Please enter a question.")
    else:
        df = st.session_state.df
        total_spent = df[df.is_expense].amount.sum()
        top_categories = (
            df[df.is_expense]
            .groupby("spend_category").amount.sum()
            .sort_values(ascending=False)
            .head(3)
            .to_dict()
        )
        trend = (
            df[df.is_expense]
            .groupby(df.datetime.dt.to_period("M"))
            .amount.sum()
            .to_dict()
        )
        threshold = df[df.is_expense].amount.quantile(0.75)
        wasteful = df[(df.is_expense) & (df.amount > threshold)][
            ["datetime", "merchant", "amount"]
        ].to_dict(orient="records")

        top2 = list(top_categories.items())[:2]
        top2_cat_str = "\n".join(f"- {cat}: â‚¹{amt}" for cat, amt in top2)
        months = sorted(trend.keys())
        if len(months) >= 2:
            prev, last = months[-2], months[-1]
            delta = trend[last] - trend[prev]
            trend_delta = f"â‚¹{abs(delta):.2f} {'â†‘' if delta>0 else 'â†“'}"
        else:
            trend_delta = "N/A"
        top_waste = sorted(wasteful, key=lambda r: r["amount"], reverse=True)[:3]
        wasteful_str = "\n".join(
            f"- {r['datetime'].strftime('%Y-%m-%d')}: {r['merchant']} (â‚¹{r['amount']})"
            for r in top_waste
        )

        # Build richer prompt for the model
        prompt = f"""
        You are a sharp personal finance coach. Based on the following spending data, give clear, actionable advice.

        Total spent: â‚¹{total_spent}
        Top categories:
        {top2_cat_str}
        Trend change: {trend_delta}
        High-value transactions:
        {wasteful_str}

        User question: {user_input}

        Instructions:
        1. Give two behavioural insights based on spending patterns.
        2. Suggest one practical way to reduce monthly spend.
        3. Recommend one bold action for long-term financial control.

        Format your response in markdown with clear headings and bullet points.
        """

        st.subheader("ðŸ’¬ Advice (streaming)")
        advice_container = st.empty()
        full_text = ""

        model = st.session_state.model
        tokenizer = st.session_state.tokenizer

        with st.spinner("Thinking..."):
            for chunk in stream_generate(model, tokenizer, prompt, max_new_tokens=512):
                full_text += chunk
                advice_container.write(full_text)

        # PDF download
        pdf_bytes = generate_advice_pdf(full_text)
        st.download_button(
            label="ðŸ“„ Download advice as PDF",
            data=pdf_bytes,
            file_name="smart_spending_advice.pdf",
            mime="application/pdf",
        )


